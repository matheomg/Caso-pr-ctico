# -*- coding: utf-8 -*-
"""Introduccion a modelos de aprendizaje supervisados

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QyEk7FHhsTt4L5E_9RCay5mP7HIoIFP4
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'pima-indians-diabetes-database:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F228%2F482%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240815%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240815T061941Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D270ec7cbeeb839f318d2229a55d3b8e86056080268ac40ae4635ee73f9382fed7a917b31776c9852c573fcda898a12d269601a3620025a897384e5dbf1687ff4f8a7c797d4578c1f0ddd4aa285824d18088f587ec03fd86b4ce3926e086a07a78b36c925d13682d194ac581b66bfb75d3dd6d82adc23ec61dc78d2494d8c01c74d03cf0864fd50612e7422c1104340df8e22b9867d291e6ad8e5820f79eff893158bb2677f846528011124f92599f3976b318a208903cbe31298be5e9c4966ea8d658052d9fb86afd9a80e7bf9acfa63b8e84531fb87b7fbad85c2998bedb2160bdb92fe72890b9928d1b1e30b30da023c9c14e4cd63e04604748868789e8e2c'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# Haciendo el setup del entorno

En este notebook aprenderemos sobre **modelos de aprendizaje supervisado**. Utilizaremos los datos de [Pima Indians](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) en el que **intentaremos predecir si un paciente tiene diabetes** o no, utilizando datos sobre su nivel de glucosa en sangre, presión arterial, índice de masa corporal, etc. Se pueden ver todos los datos en el link al dataset que se incluye arriba.

Debajo importaremos las librerías que vamos a utilizar para este problema. Como siempre, las librerías se indican al principio del notebook, para que la persona que quiera utilizar nuestro código pueda acceder rápidamente a todos los imports. Pero también repetiremos los imports cuando vayamos a utilizar la librería, solo para que el usuario sepa de donde ha salido la librería.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# Introducción al aprendizaje supervisado

En el módulo anterior vimos los [modelos de aprendizaje no supervisado](https://www.kaggle.com/code/micheldc55/introduccion-al-clustering-con-python-y-sklearn). El aprendizaje no supervisado se basa en encontrar relaciones entre los datos que permiten agrupar los datos en grupos según alguna medida de distancia. Debemos hacer este proceso porque no tenemos los datos etiquetados por lo que no tenemos una medida para contrastarlos con la realidad, así que tendremos libertad para elegir las agrupaciones que más "convenientes" sean.

En el aprendizaje **supervisado** sucede lo contrario, tenemos datos etiquetados y queremos entrenar un modelo predictivo que nos permite predecir esas etiquetas dados los datos (similares a los datos con los que se ha entrenado el modelo). La etiqueta puede ser una variable continua o una categoría y tendremos modelos para atacar cada uno.

## Problemas de clasificación:

El problema de intentar predecir una categoría (ya sea una o múltiples categorías) se conoce como problema de **clasificiación**. En este caso intentaremos predecir a qué categoría corresponde un conjunto de datos. Para este notebook, veremos en detalle este tipo de problema y cómo podemos resolverlo con distintos modelos de aprendizaje.

En términos generales, seremos mucho más precisos si tenemos que modelar el comportamiento de una categoría que de una variable continua, por lo que veremos que muchas veces, incluso los problemas de predecir variables numéricas se pueden discretizar y convertir en problemas de clasificación.

En este caso, veremos un caso típico de un problema de clasificación. **Queremos predecir si una integrante de una muestra que representa a una población, en este caso de Indias Pima, tiene diabetes**. Queremos hacer esto a partir de múltiples variables que tenemos de cada uno de los pacientes:

- Pregnancies: Número de embarazos que ha tenido en su vida
- Glucose: Nivel de concentración de glucosa en sangre
- BloodPressure: Presión arterial
- SkinThikness: Espesor de piel a la altura del triceps
- Insulin: Respuesta a dosis de insulina en 2 horas
- BMI: Índice de masa corporal
- DiabetesPedigreeFunction: Presencia de diabetes en ascendencia directa
- Age: Edad del paciente
- Outcome: Variable que queremos predecir --> 1 = Tiene diabetes, 0 = no tiene diabetes

## Cómo funciona el aprendizaje en general

Lo que llamamos "aprendizaje" en Machine Learning es en realidad la minimización de una función de coste. Los algoritmos de ML tienen definida una función de coste, que intenta penalizar al modelo por fallar en las predicciones. Los modelos también tienen una función de optimización, que permite ajustar las predicciones cuando el modelo falla.

El concepto de la función de pérdida es sencillo: si el modelo acierta en la predicción la función de coste no debe penalizar al modelo, pero en caso de que la predicción falle, la función deberá penalizar de tal forma que el modelo intente no cometer esos errores. Idealmente, cuanto más se aparte el modelo del valor que debería predecir, mayor será la penalización.

Existe un paralelismo muy fuerte entre el método de aprendizaje de los modelos de Machine Learning y el aprendizaje a través de ejemplos en los seres humanos. Si pensamos nuestras formas de aprender, tenemos dos grandes recursos a la hora de aprender nueva información:
- Conocimiento **deductivo**: Comprendemos los principios de funcionamiento de algo y desarrollamos un modelo de como se comportará el proceso si se cumplen ciertos supuestos.
- Conocimiento **inductivo**: Vemos varios ejemplos de algo que sucede y generamos una intuición del comportamiento del proceso. Por ejemplo, nadie nos enseña cuales son las características que definen al perro inherentemente, pero si vemos a un chihuahua y a un gran danés, sabemos que ambos son perros. Esto es porque en base a miles de experiencias creamos una intuición de lo que es ser "perro".

En el Machine Learning, el modelo se entrena (aprende) a partir de muchas instancias de una población. Alimentamos al modelo con casos en los que sucede o no sucede algo, o distintos ejemplos de precios de venta en función de varias variables (que llamamos features o características). En base a estos ejemplos, el modelo intentará aproximar la función que determina el valor continuo si es un problema de regresión, o la frontera de separación si es un problema de clasificación.

Nuevamente se mezclan dos conceptos para que esto funcione, el primero es que lo que buscamos es minimizar una función de coste, de manera que el modelo establezca esta frontera minimizando la diferencia entre las predicciones del modelo y los valores que realmente toma la variable que queremos predecir. Y también necesitamos las **etiquetas**, que nos dicen cual es el valor real que toma la variable para cada conjunto de valores de las variables.

# Importando los datos

Debajo importamos los datos del dataset de Pima Indians de Kaggle (el link a los datos está más arriba). Intentaremos generar rápidamente un modelo sin hacer nada con los datos y veremos qué sucede. Tener en cuenta que los datos en Kaggle generalmente están ya preprocesados y en la práctica seguramente no podamos hacer esto sin preprocesar primero, pero la idea de este problema es ir aprendiendo junto con el ejemplo.
"""

df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')
df.head()

df.info()

df.describe().T

"""# Introducción a los modelos de Machine Learning

Ahora que tenemos datos etiquetados (la variable "Outcome" es la que queremos predecir), podemos entrenar un modelo para predecir el valor de esta variable en función de los valores que toman las variables que tenemos en el problema. Veremos un modelo muy sencillo para realizar esta primera aproximación, que es el modelo de K Nearest Neighbors (KNN).

Este modelo es basa en someter a votación cual será el valor de la variable dados los demás registros. La votación se hace con los "k" puntos más próximos, por lo que debemos establecer **dos valores para que el algoritmo pueda funcionar**: La cantidad de puntos a considerar (k) y la métrica de distancia que queremos utilizar para detrminar cuales son los puntos más cercanos.

Una vez que hemos encontrado los puntos más cercanos, el modelo de KNN de clasificación asigna la clase con más representación en esos K puntos más cercanos. Si el modelo es de regresión simplemente tomamos el promedio de los K valores más cercanos.

<img src="attachment:77e783d5-9fde-4bf2-9760-c1448c816861.png" width="300px" height="300px">

En la imagen vemos un punto rojo que sería el que queremos predecir, y en función del valor de K el punto pertenecerá a una clase u otra. Si tomamos K=3 sus 3 vecinos más cercanos pertenecen 2 a la categoría morado y uno a la categoría amarillo, por lo que asignaremos el punto rojo a la categoría morado. Si elegimos K=6 tendremos 4 amarillos y 3 morados por lo que asignaríamos el punto rojo a la categoría amarillo.

Vemos entonces que la elección del valor de K en este modelo es muy importante. Un valor **muy bajo de K** será **muy sensible al ruido y a outliers**. Un modelo de KNN con un valor **muy alto de K** no será muy sensible a cambios en los datos y no aproximará bien las fronteras entre un conjunto y otro.

## Pasos para construir un modelo de KNN

El modelo de KNN es "lazy" (perezoso) por naturaleza. Esto quiere decir que al entrenarlo en realidad no se calcula nada y veremos que ejecutar el método ".fit()" sobre el modelo es instantáneo. Esto es porque el modelo solamente podrá calcular la distancia cuando le pasemos uno o más puntos que queremos clasificar.

En la práctica el modelo de KNN está aprendiendo de memoria el conjunto de entrenamiento con el que lo entrenamos, y solo cuando le pasamos un valor que no ha visto puede calcular todas las distancias respecto de ese punto o puntos.

El funcionamiento del modelo es muy sencillo y se puede resumir en estos pasos:

**1)** Se define la **medida de distancia** y el número **"K" de vecinos más cercanos** que se utilizarán para definir la categoría.

**2)** Se calculan las **distancias** de todos los puntos a categorizar a todos los puntos en el dataset, utilizando la **métrica de distancia seleccionada**, generalmente la **distancia euclidea**.

**3)** Se toman los **K valores más cercanos a cada punto y se asigna la categoría que tenga mayor representación**. En problemas de clasificación binarios (2 categorías) se suele elegir un valor de K impar para que haya desempate. En caso de empates se seleccionará al azar el valor de la categoría.

**3b)** Qué sucede si queremos hacer regresión? ---> Lo mismo, solamente que al tomar los K valores más cercanos, en lugar de tomar la categoría que tenga más representación, se toma la media de los K vecinos más cercanos.

### Características de los modelos KNN:

- El algoritmo es muy sencillo de programar e implementar. Pertenece a la categoría Lazy Learning (Aprendizaje Vago): este tipo de algoritmos no entrena un modelo, es decir, no se optimizan unos pesos, sino que simplemente compara como de parecidos son los puntos que conocemos, con los puntos nuevos, para obtener así una predicción.

- Es no paramétrico (no hace suposiciones sobre la distribución que siguen los datos, a diferencia de por ejemplo, un modelo lineal)

- KNN es local: asume que la clase de un dato depende sólo de los k vecinos mas cercanos (no se construye un modelo global)

- Si se quiere hace regresión, calcular la media de los k vecinos

- Muy sensible a los atributos irrelevantes y la maldición de la dimensionalidad

- Muy sensible al ruido

## El problema principal: El consumo de memoria

El algoritmo de K Nearest Neighbors tiene un gran problema respecto a los demás algoritmos que vamos a ver y es su alto consumo de memoria. **A qué se debe esto?** A que debemos **almacenar todos los datos de entrenamiento** (para calcular las distancias) y luego para cada predicción debemos **calcular las distancias de ese punto a todos los puntos** del conjunto de entrenamiento.

El algoritmo tiene más aplicabilidad en conjuntos de datos pequeños, en los que podemos permitirnos almacenar en memoria todos los datos y calcular esas distancias. Tener en cuenta que cuanto mayor sea nuestro conjunto que le pasemos al modelo a la hora de predecir, más tiempo tardará.

Así como vimos en el algoritmo de [K-Means clustering](https://www.kaggle.com/code/micheldc55/introduccion-al-clustering-con-python-y-sklearn) y [clustering jerárquico](https://www.kaggle.com/code/micheldc55/cluster-jer-rquico-y-dbscan-customer-segmentation), la medida de distancia entre los puntos es un factor clave en este modelo. Decimos que el algoritmo requiere que los datos estén en un espacio métrico, lo que significa básicamente que debemos definir la métrica de distancia para poder utilizarlo. Por lo que la elección de la medida de distancia puede influir muchísimo en el resultado.

La dependencia en la medida de distancia crea otro problema, y es el problema que también hemos visto, llamado **problema de escala**. Cuando hay tal dependencia de la distancia, la escala en la que varían los datos se vuelve muy relevante, por lo que debemos reescalar antes de utilizar KNN.

## Selección del valor de K:

El valor de K es uno de los pocos hiperparámetros que tendrá este modelo. Es un parámetro que elegimos cuando instanciamos el modelo, junto con la métrica de distancia que queremos utilizar. Por defecto sklearn utiliza la distancia de Minkowski y p, (la potencia de Minkowski) como 2. Cuando p=2, la distancia de Minkowski es igual a la distancia Euclidea.

```python
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor

knnc = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
knnr = KNeighborsRegressor(n_neighbors=5, metric='minkowski', p=2)
```

Para seleccionar el valor de K que mejor se ajusta a los datos (k = n_neighbors en el modelo de sklearn), ejecutamos el algoritmo de KNN con múltiples valores de K y vemos cómo se desempeña el modelo con cada valor. Seleccionaremos el valor de K que prediga mejor los valores que tenemos en la variable target. Una vez que tenemos el mejor valor de K, probamos con el conjunto de test.

Cuanto mayor sea el valor de K, más datos tendremos en cuenta para tomar la decisión. Esto hace que en las fronteras de cambio de categorías, el cambio sea menos abrupto con K grandes, pero se diluye la influencia local. Por otro lado, cuando utilizamos valores de K pequeños, nuestro modelo será más abrupto en las zonas de frontera, pero será muy sensible al ruido.

## KNN ajustado:

Existe una manera de "ajustar" el algoritmo de K vecinos más cercanos utilizando una normalización basada en distancia. La distancia puede ser cualquiera (Minkowski, distancia Euclidea, distancia de Manhattan, etc), lo importante es que el "voto" de cada punto será proporcional a 1/d. Por lo tanto, el voto de cada punto dentro de los K más cercanos será propocional al inverso de la distancia.

En general, tanto para el modelo de KNN normal como para el modelo de KNN ajustado, se recomienda utilizar valores de K impares, principlamente si hay dos clases. Esto se debe a que el número impar garantiza (en el KNN normal) que haya desempate. Si elegimos un número de clases par y las clases en los K vecinos están divididas en un número igual de ocurrencias para cada clase, el modelo seleccionará aleatoriamente la clase correspondiente para ese punto.

## Análisis de Fronteras:

En la imagen que vemos debajo, podemos ver un ejemplo de aplicación de KNN eligiendo dos valores de K (K=1 o K=7). En el caso de K=1, el modelo es muy sensible al ruido, ya que el punto más cercano es el único que define la clase. En el caso de K=7, vemos que la frontera es más suave. Esto es porque como estamos teniendo más puntos en cuenta, necesitamos mayor presencia de puntos de la otra clase para "cambiar" la decisión.

Si lo llevamos al caso límite, cuando K tiende a infinito (o al número total de registros en el conjunto de entrenamiento), veremos que el resultado de todos los puntos predichos será 1 o 0, dependiendo de cuantos puntos haya de cada clase. Básicamente solo importa la generalidad total, cuantos puntos hay en total de cada clase.

![image.png](attachment:dc2a73bc-0ba7-489b-ae51-7e776c505b4b.png)

# Overfitting (sobreajuste) en modelos de KNN:

Primero que nada, repasamos: **¿Qué significa sobreajuste (overfitting)?** Significa que un modelo aprende los detalles y el ruido en los datos, y no aprende las "generalidades" de los datos de entrenamiento en la medida en que impacta negativamente el rendimiento del modelo en nuevos datos.

![image.png](attachment:8e3bf87b-71ba-43ce-92b9-6372186e5889.png)

La línea verde representa un **modelo sobreajustado** y la línea negra representa un **modelo regularizado**. Si bien la línea verde sigue mejor los datos de entrenamiento, es demasiado dependiente de esos datos y es probable que tenga un error más alto en datos nuevos y no vistos por el modelo, en comparación con la línea negra.

El **desajuste** se refiere a un modelo que **no puede modelar los datos de entrenamiento** ni **generalizarse** a nuevos datos. Por ejemplo, el uso de un algoritmo lineal en datos no lineales tendrá un rendimiento deficiente.

Tenemos que encontrar la media ideal para obtener un modelo que pueda generalizar bien, sobre todo con nuevos datos. El truco está en aprender a generalizar y evitar aprender "demasiado", evitando aprender el ruido y previniendo el sobreajuste del modelo.

## Relación entre K y el sobreajuste:

En la tabla que vemos debajo, veremos dos gráficas. La primera muestra el error del conjunto de **training**. Generalmente, el conjunto de training **NO** empeora en su entrenamiento. Nuestro modelo tiende a aprender cada vez mejor el conjunto de train (esto es un problema, estamos aprendiendo el ruido!). La gráfica que debemos mirar es la del conjunto de **test**. Vemos que el error de test disminuye hasta cierto punto y luego comienza a subir de nuevo. Este es el proceso que debemos seguir para encontrar el valor de **K óptimo**.

En K = 1, el error de entrenamiento es 0 porque el punto de cierre de cualquier punto de datos de  entrenamiento es él mismo. El error de Test, cuando K = 1 es muy alto, lo cual es normal porque
tenemos sobreajuste.

A medida que aumentamos el valor de K, la tasa de error de Test disminuye hasta alcanzar su punto mínimo, después de lo cual el error comienza a aumentar nuevamente. Básicamente , el problema para encontrar la mejor K es un problema de optimización , encontrar el valor mínimo en una curva.

Esto se llama el método del codo , porque la curva de error de prueba se parece a un codo. Conclusión: para encontrar el mejor K use el método del codo y encuentre el mínimo en la curva. Esto se puede hacer fácilmente mediante la fuerza bruta, ejecutando el modelo varias veces, cada vez aumentando el valor de K.

![image.png](attachment:b9cc17d4-bd2a-49ce-95d7-0dc38bab4c01.png)
"""

